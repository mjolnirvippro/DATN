services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports: ["11434:11434"]
    volumes: ["./ollama:/root/.ollama"]
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_KV_CACHE_TYPE=cpu
      - OLLAMA_MAX_LOADED_MODELS=1
    restart: unless-stopped

  litellm:
    image: litellm/litellm:v1.78.7-nightly
    platform: "linux/amd64"
    container_name: litellm
    ports: ["4000:4000"]
    environment:
     # - LOG_LEVEL=info
      - LITELLM_LOG=info
      - LITELLM_CONFIG=/app/config.yaml
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - ./litellm_config.yaml:/app/config.yaml:ro
     #  - ./litellm_config.yaml:/app/litellm_config.yaml:ro
    command: ["--port", "4000", "--config", "/app/config.yaml"]
#command: ["--port", "4000", "--config", "/app/litellm_config.yaml"]   
    depends_on: [ollama]
    restart: unless-stopped

  #tei:
   # image: ghcr.io/huggingface/text-embeddings-inference:cpu-0.5
   # container_name: tei
   # command: ["--model-id","BAAI/bge-m3"]
   # ports: ["8080:80"]
   # environment:
   #   - HF_API_TOKEN=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
   #  restart: unless-stopped


  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports: ["6333:6333","6334:6334"]
    volumes: ["./qdrant:/qdrant/storage"]
    restart: unless-stopped


